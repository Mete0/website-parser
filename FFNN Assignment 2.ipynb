{
  "metadata" : {
    "kernelspec" : {
      "display_name" : "Python 2",
      "language" : "python",
      "name" : "python2"
    },
    "language_info" : {
      "file_extension" : ".py",
      "mimetype" : "text/x-python",
      "name" : "python"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 2,
  "cells" : [ {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "![MLU Logo](https://drive.corp.amazon.com/view/bwernes@/MLU_Logo.png?download=true)" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Week 2\n\nIn this week we work with two layer neural networks, and backpropegation.  Rather than a classification problem like last week, this week we will attempt image completion, which is a high dimensional regression task.\n\n##  Supplemental Reading\n*  [This video](https://www.youtube.com/watch?v=Ilg3gGewQ5U) on backpropegation.\n*  [This book chapter](http://neuralnetworksanddeeplearning.com/chap2.html) on which our presentation, and the above video, are based.\n*  Chapter 12 on multi-layer networks and backprop in Python Machine Learning book.\n*  Chapter 6 up to 6.2.1.2 in [Deep Learning book](http://www.deeplearningbook.org/)." ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Doing it by hand!\n\nPart of me wants to apologize, because a significant fraction of this assignment is all done by hand on paper doing the work of an algorithm.  However understanding how this algorithm works is fundamental to understanding why various choices are made in the design of neural networks.  Topics we get to later: batch normalization, network initialization, and activation function selection all are founded on the behavior you expect to see in the intermediate steps when running this algorithm.  So get out a pencil and a piece of paper, and be prepared to feel like an algorithm!  We'll get around to coding things later.\n\n### Question 1\nConsider the neural network with weights pictured below.\n\n![Network Image](https://drive.corp.amazon.com/view/bwernes@/Network.png?download=true)\n\nThe left neurons are the input neurons, and thus have no activation function. The hidden neurons will be assumed to have relu activations, which is to say\n$$\n\\sigma(x) = \\max\\{x,0\\} \\quad \\sigma'(x) = \\begin{cases}\n1 & x > 0, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\nThe output neuron will be assumed to have a linear activation function $\\sigma(x) = x$.  For simplicity, we will assume there are no biases in this network, so the ouput is just $\\sigma(w_0x_0+w_1x_1)$ and so on.  Suppose we provide input $(x_0,x_1) = (1,0)$ to this network. What is the output?" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Answer 1\n\n*(Write answer here.)*" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Question 2\n\nFor the same network as above, and the same input, suppose we know the target value is $1$, and we use the squared error loss function: $L = (output - target)^2$.  Use the backpropegation algorithm to compute one step of gradient descent with learning rate $\\eta = \\frac{1}{8}$.  What are the new weights?  What is the output after updating the weights this single step?" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Answer 2\n\n*(Write answer here.)*" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Implementing Gradient Descent\n\nLet's turn now to code and implement gradient descent by hand.  We will use the [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset which is a collection of small images.  Rather than classify these images, we will attempt to create a model that predicts the bottom half of the image based on the top half.\n\n### Question 3\n\nUse [keras.datasets.cifar10](https://keras.io/datasets/) to load our data. *At this point, do not use Keras for any of the implementation of the network - that comes later!*  \n* Plot the first 25 images in the training set to see the quality of our data.  \n* We will ignore the given labels at this time, and build our own dataset which consists of the top half of the images (which are 32 by 32 by 3 arrays in memory) for our training input and the bottom half of the images for our training output.  Please construct these two lists.  To implement the biases in the network, append the constant one to the end of the input vectors.  To keep training times fast, please only take the first 1000 images from the dataset.  Scaling the inputs to the proper range will be important later on, so make the rgb values lie in the range $[0,1.0]$ rather than $[0,255]$ being mindful of Python 2.7's integer division issues." ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "### Answer 3\n\n# CODE ANSWER HERE #" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "We now want to design our network.  We will take a similar tactic to what is known as an *autoencoder*.  This is a type of neural network that is essentially a machine-learned compression method, taking in an input, pushing it through a small bottleneck of neurons, and then re-expanding it back out into the target image.  In particular we will have the following architecture:\n* The input is a length $32*16*3+1 = 1537$ vector of the top of the image, plus a bias.\n* The second layer will be $100$ neurons (meaning we need a $1537$ by $100$ matrix of weights).  The activation function will be a sigmoid $\\sigma(x) = \\frac{1}{1+e^{-x}}$.  A nice trick to know is that one can show that $\\sigma'(x) = \\sigma(x)\\cdot(1-\\sigma(x))$ for the sigmoid, and thus computing the derivatives is easy.\n* Once the output of the second layer is computed, append a bias node (constant $1$) to get a length $101$ vector.  The weights for the output layer will then be a $101$ by $32*16*3 = 1536$ matrix.  The output activations will be linear.\n* The loss will be given by the summed squared error of the rgb values in the bottom half of the image.\n\n### Question 4\nImplement the forward pass for the network.  We will get into details in a later lecture, but initializing the weights properly is important to ensure the network learns effectively.  Initialize the layers so that every weight is a standard Gaussian divided by the number of incoming edges.  For instance the first layer should be \n```python\nw1 = np.random.randn(1537,100)/np.sqrt(1537)\n```\nPlot the result of a forward pass on the first training image to see that it produces random noise as desired.  *Note: ```imshow``` interprets rgb values as taking the range [0,255] if they are integers, but [0,1.0] if floats.  Additionally values outside the range desired range causes the image to not be shown, so make sure you clamp values (using ```clip``` from numpy) to the proper range.*" ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "### Answer 4\n\n# CODE ANSWER HERE #" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "### Question 5\nImplement stochastic gradient descent via backpropegation.  Run through the dataset for 100 epochs with a learning rate of $\\eta = 0.001$, updating the weights after processing every datapoint.  Do not forget to shuffle the order of the datapoints between epochs.  Once done, plot the first 25 images with their predicted completions.  How did the network do?  A back-of-the-envelop computation shows that $100$ parameters is only enough to send a $4$ by $8$ rgb image ($4\\cdot8\\cdot3 = 96 \\approx 100$) so that is the standard you should compare against. " ]
  }, {
    "cell_type" : "code",
    "execution_count" : 0,
    "outputs" : [ ],
    "metadata" : { },
    "source" : [ "### Answer 5\n\n# CODE ANSWER HERE #" ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# A few quick remarks\n\nThis assignment let us walk through backpropegation, and show how people can use neural networks in settings beyond what you might immediately think of, and into the realm of image completion.  [Modern techniques can do vastly better](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/data/completion_sig2017.pdf), but the core principle remains the same: pass the incomplete image through a narrow bottleneck, that then generates a completed image which is then compared against the true answer." ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "# Final Project\n\nWe will finally have the tools in hand to start working on the final project next week.  For now, take a look at [this notebook](https://eider.corp.amazon.com/bwernes/notebook/output/NBDNDHO2TM) to get a feel for what we'll be doing." ]
  }, {
    "cell_type" : "markdown",
    "metadata" : { },
    "source" : [ "## Assignment Submission\nYou are now done with the assignment! Once complete, go to **Window > Versions** to create a version of this assignment. Copy the link of the versioned notebook you created by using the **hamburger menu > Share ... > Copy**. Paste that link into the submission box in Canvas, and you are done!\n\n## Notebook Commenting \nEider has launched a new experience for adding comments in read-only notebooks and it's currently in Limited Beta. You'll be able to discuss homework solutions, comment on your classmate's work, and receive feedback from your instructor/TAs! To try out this experience, share a *versioned* notebook with your instructors/TAs/classmates.\n\n### Submit feedback/ report issues \nTo submit feedback or report issues, please file a SIM by clicking [HERE](https://issues.amazon.com/issues/create?template=98e387b8-5c06-4be0-8227-768de94ddf13).\nFor people who provide feedback by 12/11/2018, we'll award an Eider Beta Tester Phonetool icon.\n\n### Not seeing the \"SHOW COMMENTS\" option in your read-only notebook?\nWe've added active S7 students in our Beta testing group. If you couldn't see the \"SHOW COMMENTS\" option in your read-only notebook, please [SIGN UP HERE](https://quip-amazon.com/ggIlANowXeHD) and we will add you into the Beta group ASAP.\n\nFor more information and updates on this feature, check out the Notebook Commenting Wiki [HERE](https://w.amazon.com/bin/view/Eider/Docs/NotebookCommenting/)." ]
  } ]
}